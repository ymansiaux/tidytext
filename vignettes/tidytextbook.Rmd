---
title: "tidytextbook"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{tidytextbook}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(tidytextbook)
# repo de l'appli : https://github.com/juliasilge/learntidytext
```


# Chapter 2 : Thank you for coming to my TED talk

The first case study of this tutorial uses a data set of TED talks created by Katherine M. Kinnaird and John Laudun for their paper “TED Talks as Data”. The specific talks we are using are from the main TED event, and the data set was curated in the summer of 2018.

There are two main pieces of R software we will use in our text analysis work throughout this tutorial, the tidyverse metapackage and tidytext. To clarify for yourself what tools you are using, load the two packages below (first tidyverse, and then tidytext) by replacing the ___ with the package names.


```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(tidyverse)
library(tidytext)
```


## TED talk transcripts
The TED talk transcripts are available to you in a dataframe called ted_talks. There are three variables in this data set:

* talk_id: the identifier from the TED website for this particular talk

* text: the text of this TED talk

* speaker: the main or first listed speaker (some TED talks have more than one speaker)


```{r}
ted_talks <- read_rds("../data/ted_talks.rds")
glimpse(ted_talks)
```

### How to tidy text data

The text data is currently in a dataframe, but it is not tidy in the sense of being compatible with tidy tools. We need to transform it so that it is in a different format, with one observation per row.

When we do text analysis, the observations we are interested in aren’t the whole talks at once, but rather individual tokens. A token is a meaningful unit of text for analysis; in many cases, this just means a single word. The process of tokenization identifies and breaks apart text into individual tokens. You can use tidytext’s unnest_tokens() function to accomplish all of this at once, both the tidying and the tokenization.

```{r}
tidy_talks <- ted_talks %>% 
  unnest_tokens(word, text)

head(tidy_talks)
```

The unnest_tokens() function transforms non-tidy text data into tidy text data. It takes three arguments:

the input dataframe that contains your text (often you will use the pipe %>% to send this argument to unnest_tokens()),
the output column that you want to unnest to, and
the input column that you want to unnest from.

What did unnest_tokens() do here? Instead of having 992 rows and reading each talk across the line in text, we now have 2,005,342 rows and can read each talk down the column in word. We have tokenized and tidied the text, as well as a few other transformations:

Other columns have been retained.
Punctuation has been stripped out.
Words have been converted to lower-case.
These are defaults in the function that can be changed, if not appropriate to your analysis.


### Tokenize to bigrams

We said before that tokenization is the process of identifying and breaking apart text into tokens, meaningful units of text; those meaningful units of text are most often single words in text analysis but they do not have to be! We can move beyond single words to other kinds of tokens, like n-grams. An n-gram is a consecutive sequence of n words. Let’s look at these TED talks and tokenize to bigrams, n-grams of order 2.

Use the same function for tokenizing and tidying to create the bigrams.

```{r}
ted_bigrams <- ted_talks %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

glimpse(ted_bigrams)
```

### Most common TED talk words
Let’s go back to single words. Now that our data in a tidy format, a whole world of analysis opportunity has opened up for us. We can start by computing term frequencies in just one line. What are the most common words in these TED talks?

Use count() to find the most common words.

```{r}
tidy_talks %>%
  count(word, sort = TRUE)
```


### Removing stop words

Words like “the”, “and”, and “to” that aren’t very interesting for a text analysis are called stop words. Often the best choice is to remove them. The tidytext package provides access to stop word lexicons, with a default list and then other options and other languages.

First, run the code the way it is.
Next, try out the language argument, which takes two-letter language abbreviations like "es".

```{r}
get_stopwords()
get_stopwords(language = "fr")
```

```{r}
tidy_talks %>%
  anti_join(get_stopwords()) %>%
  count(word, sort = TRUE)
```

### Visualize top words

```{r}
tidy_talks %>%
  # remove stop words
  anti_join(get_stopwords()) %>%
  count(word, sort = TRUE) %>% 
  slice_max(n, n = 20) %>%
  mutate(word = reorder(word, n)) %>%
  # put `n` on the x-axis and `word` on the y-axis
  ggplot(aes(n, word)) +
  geom_col()
```

### Compare TED talk vocabularies

One of my favorite approaches to text analysis is to compare how different people or groups use language. There are lots of different ways to do this, but you can start with plain old word counts! Let’s look at two TED talk speakers, Jane Goodall and Temple Grandin, and count up the words they used in their TED talks.

If you want to explore other speakers, switch out for different speakers’ names from the data set and hit “Run Code”, after finishing the exercise.

Use filter() to keep only the words spoken by Jane Goodall and Temple Grandin.

Remove the default list of stop words.

Use count() with two arguments to count up the term frequencies by speaker and word. (These first three steps could actually be completed in any order but this makes most sense to me.)

Come back and filter() again to only keep words spoken at least 10 times by both women.

The function pivot_wider() from tidyr pivots the long, tidy dataframe to a wide dataframe so we can more easily compare the two speakers’ word counts.

```{r}
tidy_talks %>%
  filter(speaker %in% c("Jane Goodall", "Temple Grandin")) %>%
  # remove stop words
  anti_join(get_stopwords()) %>%
  # count with two arguments
  count(speaker, word, sort = FALSE) %>%
  group_by(word) %>%
  filter(sum(n) > 10) %>%
  ungroup() %>%
  pivot_wider(names_from = "speaker", values_from = "n", values_fill = 0) 
```

## Visualize vocabulary comparison

```{r}
library(ggrepel)

tidy_talks %>%
  filter(speaker %in% c("Jane Goodall", "Temple Grandin")) %>%
  anti_join(get_stopwords()) %>%
  count(speaker, word) %>%
  group_by(word) %>%
  filter(sum(n) > 10) %>%
  ungroup() %>%
  pivot_wider(names_from = "speaker", values_from = "n", values_fill = 0) %>%
  ggplot(aes(`Jane Goodall`, `Temple Grandin`)) +
  geom_abline(color = "gray50", size = 1.2, alpha = 0.8, lty = 2) +
  # use the special ggrepel geom for nicer text plotting
  geom_text_repel(aes(label = word)) +
  coord_fixed()
```


# Chapter 3 : Shakespeare

```{r}
shakespeare <- read_rds("../data/shakespeare.rds")
```


```{r}
shakespeare %>% 
  glimpse()

shakespeare %>%
  count(genre)

shakespeare %>%
  count(genre, title)
```

## Unnesting from text to word

```{r}
tidy_shakespeare <- shakespeare %>%
  group_by(title) %>%
  mutate(linenumber = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, text)

tidy_shakespeare %>% 
  # count to find out how many times each word is used
  count(word, sort = TRUE)
```

## Sentiment lexicons

Sentiment analysis is a way to measure the attitudes and opinions expressed in text, and can be approached in multiple ways. A common approach is to use sentiment lexicons, lists of words that have been curated and scored in some way. Lexicons are typically created by NLP researchers and some have licenses that restrict their use, for example in commercial settings. The "bing" lexicon of Hu and Liu (2004) is a general purpose English lexicon (which can be used in commercial settings with attribution) that categorizes words as either positive or negative.

```{r}
get_sentiments("bing")
```

## Sentiment analysis of Shakespeare

```{r}
shakespeare_sentiment <- tidy_shakespeare %>%
  # implement sentiment analysis with the "bing" lexicon
  inner_join(get_sentiments("bing")) 

shakespeare_sentiment %>%
  # find how many positive/negative words each play has
  count(title, sentiment)
```

## Tragedy or comedy ?

Which plays have a higher percentage of negative words? Do the tragedies have more negative words than the comedies?


```{r}
sentiment_counts <- tidy_shakespeare %>%
  # implement sentiment analysis using the "bing" lexicon
  inner_join(get_sentiments("bing"))  %>%
  # count the number of words by title, genre, and sentiment
  count(title, genre, sentiment)

sentiment_counts %>%
  group_by(title) %>%
  # find the total number of words in each play
  mutate(total = sum(n),
         percent = n / total) %>%
  # filter the results for only negative sentiment
  filter(sentiment == "negative") %>%
  arrange(percent)
```

## Most common positive and negative words

```{r}
word_counts <- tidy_shakespeare %>%
  inner_join(get_sentiments("bing"))  %>%
  # count by word and sentiment
  count(word, sentiment)

top_words <- word_counts %>%
  # group by sentiment
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n))

ggplot(top_words, aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ sentiment, scales = "free")
```

## Sentiment changes through a play

In the last set of exercises in this case study, you will examine how sentiment changes through the narrative arcs of these Shakespearean plays. We will start by first implementing sentiment analysis using inner_join(), and then use count() with four arguments:

title, genre, an index that will section together lines of the play, and sentiment.

```{r}
tidy_shakespeare %>%
  # implement sentiment analysis using "bing" lexicon
  inner_join(get_sentiments("bing"))  %>%
  # count using four arguments
  count(title, genre, index = linenumber %/% 70, sentiment)
```

## Visualizing narrative arcs

Now you will build on the code from the previous exercise and continue to move forward to see how sentiment changes through these Shakespearean plays.


```{r}
tidy_shakespeare %>%
  inner_join(get_sentiments("bing")) %>%
  count(title, genre, index = linenumber %/% 70, sentiment) %>%
  # pivot sentiment and n wider
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  # use mutate to compute net sentiment
  mutate(sentiment = positive - negative) %>%
  # put index on x-axis, sentiment on y-axis, and map comedy/tragedy to fill
  ggplot(aes(index, sentiment, fill = genre)) +
  # make a bar chart with geom_col()
  geom_col() +
  # make small multiples for each title with facet_wrap()
  facet_wrap(~ title, scales = "free_x")
```


# Chapter 4 : Newspaper headlines

```{r}
nyt_headlines <- read_rds("../data/nyt_headlines.rds")
```

```{r}
nyt_headlines %>%
  count(section, sort = TRUE)
```


## Tidying newspaper headlines

```{r}
# pipe `nyt_headlines` to the next line
tidy_nyt <- nyt_headlines %>% 
  mutate(id = row_number()) %>%
  unnest_tokens(word, headline)

  # transform the `headline` column to a `word` column
tidy_nyt

```


## Most common headline words

```{r}
tidy_nyt %>%
  count(word, sort = TRUE)
```

```{r}
tidy_nyt %>%
  anti_join(get_stopwords()) %>%
  count(word, sort = TRUE)

```

## What are the newspaper sections about?

A central question in text mining is how to quantify what different documents or categories of documents are about. One approach to measure this is to use tf-idf, term frequency-inverse document frequency. A word’s term frequency is how frequently it occurs in a document. A word’s inverse document frequency is a weight, which decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents:

If you multiply the two together, you get tf-idf, a statistic intended to measure how important a word is to a document in a collection (or corpus) of documents. The bind_tf_idf() function from tidytext can compute tf-idf for a text data in a tidy format.

Let’s look at this in the context of the NYT headlines. In our case study here, each headline is a document, and the corpus is the whole month of headlines.

Start by counting up the words in the tidy_nyt data, using three arguments: section (to keep track of the newspaper sections), id for each headline, and word.

Specify the appropriate three arguments for bind_tf_idf():

The first is the token or term, word, and the second is the document, the headline id.

The last argument is the column that contains the document-term counts, which is n, the output of count().

Because of the line arrange(-tf_idf), the output shows the highest tf-idf words in the whole data set of headlines, the most distinctive words found.

```{r}
nyt_tf_idf <- tidy_nyt %>%
  # count with three arguments
  count(section, id, word) %>%
  # there are also three arguments for `bind_tf_idf()`
  bind_tf_idf(word, id, n) %>%
  arrange(-tf_idf)

nyt_tf_idf
```

Great work! The highest tf-idf words also have both n and term frequency equal to 1, meaning that these were headlines that only had a single word in them. Not all the headlines in this data set are what we would think of as traditional headlines.

```{r}
sections <- c("Food", "Opinion", "Technology", "The Upshot")

nyt_tf_idf %>%
  filter(!near(tf, 1)) %>%
  filter(section %in% sections) %>%
  group_by(section) %>%
  slice_max(tf_idf, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  # put `tf_idf` on the x-axis, `word` on the y-axis
  ggplot(aes(tf_idf, word, fill = section)) +
  geom_col(show.legend = FALSE) +
  # make small multiples
  facet_wrap(~ section, ncol = 2, scales = "free")
```

## Tokenize headlines to bigrams

```{r}
nyt_bigrams <- nyt_headlines %>%
  mutate(id = row_number()) %>%
  # tidy/tokenize with n = 2
  unnest_tokens(bigram, headline, token = "ngrams", n = 2)

nyt_bigrams
```

## Compute tf-idf of bigrams

```{r}
bigram_tf_idf <- nyt_bigrams %>%
  # count with three arguments
  count(section, id, bigram) %>%
  bind_tf_idf(bigram, id, n) %>%
  arrange(-tf_idf)

bigram_tf_idf
```

```{r}
sections <- c("Books", "Climate", "New York", "Parenting")

bigram_tf_idf %>%
  filter(!near(tf, 1)) %>%
  filter(section %in% sections) %>%
  # what should you group by?
  group_by(section) %>%
  slice_max(tf_idf, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
  # how should the plot aesthetics be set up?
  ggplot(aes(tf_idf, bigram, fill = section)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ section, ncol = 2, scales = "free")
```

Magnificent! Remember that we computed tf-idf per headline and did not put any information about the sections into the tf-idf computation. The reason that high tf-idf bigrams about drilling are associated with climate reporting is because tf-idf is good at finding distinctive tokens in a document compared to a collection of other documents.


# Chapter 5 : Singing a different tune

You have made it to your final case study of this tutorial! This case study demonstrates again how these kinds of techniques are applicable for many diverse kinds of texts. You have analyzed TED talks, Shakespeare’s plays, New York Times headlines, and now, in this final case study, you are going to use your text mining and sentiment analysis skills on a data set of lyrics from pop songs over 50 years. All these texts are very different each other, but you are able to gain insight in each case because this approach is flexible and powerful.


```{r}
song_lyrics <- read_rds("../data/song_lyrics.rds")
```

## Tidying song lyrics

```{r}
# pipe song_lyrics to the next line
tidy_lyrics <- song_lyrics  %>% 
  # transform the lyrics column to a word column
  unnest_tokens(word, lyrics)

# print tidy_lyrics
tidy_lyrics
```

```{r}
tidy_lyrics %>%
  count(word, sort = TRUE)

```


## Words per song

```{r}
tidy_lyrics %>% 
  # count with two arguments to find the number of words used in each song each year
  count(year, song) %>% 
  # put `year` on the x-axis and `n` on the y-axis
  ggplot(aes(year, n)) + 
  # make a scatter plot with `geom_point()`
  geom_point(alpha = 0.4, size = 5, color = "darkcyan") + 
  # fit a line to the points with `geom_smooth()`
  geom_smooth(method = "lm", color = "black")
```

## Pop vocabulary over decades


```{r}
word_counts <- tidy_lyrics %>% 
  anti_join(get_stopwords()) %>% 
  count(year, word) %>% 
  # group by `year`
  group_by(year) %>%
  # create a new column for the total words per year
  mutate(year_total = sum(n)) %>% 
  ungroup() %>% 
  # now group by `word`
  group_by(word) %>% 
  # keep only words used more than 500 times
  filter(sum(n) > 500) %>% 
  ungroup()

word_counts
```

## Many models

Now we can use the new word_counts data set to train many linear models, one per word. We will load the broom package to handle the model output.

Create list-columns by nesting the word count data, nesting by word.
Use mutate() to create a new column for our models. We are training one model for each word, with year as the predictor.
NOTE: We are creating a slightly more complicated model than shown in the many models chapter, because we are modeling word counts, rather than a continuous property. The following is a generalized linear model glm() using numbers of “successes” (i.e. counts for a specific word) and “failures” (i.e. the total counts for the year) as the response; this approach works better for this kind of data.

Within the call to summarize(), use the broom function tidy() to summarize the model objects we just created for each word.
Use filter() to only keep the "year" terms in the output, since we are not interested in the intercepts.


```{r}
library(broom)

slopes <- word_counts %>%
  nest_by(word) %>%
  # create a new column for our `model` objects
  mutate(model = list(glm(cbind(n, year_total) ~ year, 
                          family = "binomial", data = data))) %>%
  summarize(tidy(model)) %>%
  ungroup() %>%
  # filter to only keep the "year" terms
  filter(term == "year") %>%
  mutate(p.value = p.adjust(p.value)) %>%
  arrange(estimate)

slopes
```


## Visualizing many models

It’s time for the last exercise in this case study on song lyrics! Let’s create a visualization for all these models we trained with plotly. The visualization we are going to create is what is called a volcano plot, comparing effect size and statistical significance. You can use your mouse to see the words in the tooltip of the interactive plot

```{r}
library(plotly)

p <- slopes %>%
  # put `estimate` on the x-axis and `p.value` on the y-axis
  ggplot(aes(estimate, `p.value`, label = word)) +
  geom_vline(xintercept = 0, lty = 2, size = 1.5, alpha = 0.7, color = "gray50") +
  # make a scatter plot using `geom_point()`
  geom_point(color = "darkcyan", alpha = 0.5, size = 2.5) +
  scale_y_log10()

# call ggplotly() on the plot we made above
ggplotly(p)
```

Delightful! The effect size on the x-axis tells us whether a word is being used more (positive) or less (negative) as time passes. The p-value on the y-axis tells us about the significance of the result for that particular word. In earlier decades, song lyrics were more likely to address love/lovin’/loves while in more recent decades, song lyrics were more likely to include profanity.


# Chapter 6 : Going further

Congratulations! You have finished these four case studies and learned so much about how to analyze text data with tidy data principles. We’ve linked to the documentation site for tidytext several times throughout this tutorial, but there are also other resources available for you to extend your learning.

Learn more about exploratory data analysis for text
To dive deeper into how to summarize, visualize, and thoroughly explore text data, check out my book Text Mining with R: A Tidy Approach with my coauthor David Robinson.

Learn about supervised machine learning for text
To learn how to build reliable and appropriate machine learning models with text data, check out my book Supervised Machine Learning for Text Analysis in R with my coauthor Emil Hvitfeldt.

Explore other approaches to text analysis in R
Using the tidytext package isn’t the only way to approach your text analysis needs. Other R packages I have used and like include quanteda, cleanNLP, and spacyr. The CRAN task view on natural language processing contains many more R packages.

Learn more about learnr
This tutorial was made with the learnr package in R. See the learnr introduction and some example tutorials here: https://rstudio.github.io/learnr/

